#!/usr/bin/env python3
"""
Claude SEO Agent - AIé©±åŠ¨çš„SEOåˆ†æå’Œä¼˜åŒ–åŠ©æ‰‹

è¿™ä¸ªAgentèƒ½å¤Ÿï¼š
1. è‡ªåŠ¨åˆ†æç½‘ç«™SEOçŠ¶å†µ
2. æä¾›è¯¦ç»†çš„ä¼˜åŒ–å»ºè®®
3. ä¸ç”¨æˆ·äº¤äº’ç¡®è®¤ä¼˜åŒ–æ–¹æ¡ˆ
4. ç”Ÿæˆå¯æ‰§è¡Œçš„ä¼˜åŒ–ä»£ç 
5. è·Ÿè¸ªä¼˜åŒ–æ•ˆæœ

ä½œè€…: Claude AI
ç‰ˆæœ¬: 1.0.0
"""

import json
import re
import requests
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from urllib.parse import urljoin, urlparse
import time
from bs4 import BeautifulSoup
import csv
from datetime import datetime

@dataclass
class SEOIssue:
    """SEOé—®é¢˜æè¿°"""
    category: str
    severity: str  # critical, high, medium, low
    title: str
    description: str
    recommendation: str
    code_solution: Optional[str] = None
    impact_score: float = 0.0

@dataclass
class SEOMetric:
    """SEOæŒ‡æ ‡"""
    name: str
    current_value: float
    target_value: float
    unit: str
    status: str  # good, warning, critical

class SEOOptimizerAgent:
    """Claude SEOä¼˜åŒ–Agent"""

    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip('/')
        self.parsed_url = urlparse(self.base_url)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Claude-SEO-Agent/1.0 (SEO Analysis Bot)'
        })

        # SEOåˆ†æç»“æœ
        self.issues: List[SEOIssue] = []
        self.metrics: List[SEOMetric] = []
        self.analysis_timestamp = datetime.now()

    def print_header(self):
        """æ‰“å°Agentå¤´éƒ¨ä¿¡æ¯"""
        print("=" * 80)
        print("ğŸ¤– Claude SEO Agent - AIé©±åŠ¨çš„SEOåˆ†æå’Œä¼˜åŒ–åŠ©æ‰‹")
        print("=" * 80)
        print(f"ğŸ“Š åˆ†æç›®æ ‡: {self.base_url}")
        print(f"â° åˆ†ææ—¶é—´: {self.analysis_timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)

    def get_user_confirmation(self, message: str, default: bool = True) -> bool:
        """è·å–ç”¨æˆ·ç¡®è®¤"""
        suffix = " [Y/n]" if default else " [y/N]"
        response = input(f"\nâ“ {message}{suffix}: ").strip().lower()

        if not response:
            return default

        return response in ['y', 'yes', 'æ˜¯', 'ok', 'ç¡®å®š']

    def get_user_choice(self, question: str, options: List[str]) -> int:
        """è·å–ç”¨æˆ·é€‰æ‹©"""
        print(f"\nâ“ {question}")
        for i, option in enumerate(options, 1):
            print(f"   {i}. {option}")

        while True:
            try:
                choice = input("è¯·é€‰æ‹© (è¾“å…¥æ•°å­—): ").strip()
                choice_idx = int(choice) - 1
                if 0 <= choice_idx < len(options):
                    return choice_idx
                else:
                    print("âš ï¸  è¯·è¾“å…¥æœ‰æ•ˆçš„é€‰é¡¹æ•°å­—")
            except ValueError:
                print("âš ï¸  è¯·è¾“å…¥æ•°å­—")

    def analyze_page(self, url: str) -> Optional[BeautifulSoup]:
        """åˆ†æå•ä¸ªé¡µé¢"""
        try:
            print(f"ğŸ” æ­£åœ¨åˆ†æ: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            print(f"âœ… é¡µé¢åˆ†æå®Œæˆ: {url}")
            return soup

        except Exception as e:
            print(f"âŒ é¡µé¢åˆ†æå¤±è´¥: {url} - {str(e)}")
            return None

    def check_title_tags(self, soup: BeautifulSoup, url: str) -> None:
        """æ£€æŸ¥Titleæ ‡ç­¾"""
        title_tag = soup.find('title')

        if not title_tag:
            self.issues.append(SEOIssue(
                category="Metaæ ‡ç­¾",
                severity="critical",
                title="ç¼ºå°‘Titleæ ‡ç­¾",
                description=f"é¡µé¢ {url} æ²¡æœ‰Titleæ ‡ç­¾",
                recommendation="æ·»åŠ æè¿°æ€§çš„Titleæ ‡ç­¾ï¼ŒåŒ…å«ä¸»è¦å…³é”®è¯ï¼Œé•¿åº¦æ§åˆ¶åœ¨50-60å­—ç¬¦",
                code_solution='<title>é¡µé¢æ ‡é¢˜ - ä¸»è¦å…³é”®è¯</title>',
                impact_score=10.0
            ))
            return

        title_text = title_tag.get_text().strip()
        title_length = len(title_text)

        if title_length < 30:
            self.issues.append(SEOIssue(
                category="Metaæ ‡ç­¾",
                severity="high",
                title="Titleæ ‡ç­¾è¿‡çŸ­",
                description=f"Titleæ ‡ç­¾ '{title_text}' é•¿åº¦ä¸º {title_length} å­—ç¬¦ï¼Œå°‘äºæ¨èçš„30å­—ç¬¦",
                recommendation="å¢åŠ Titleæ ‡ç­¾é•¿åº¦ï¼ŒåŒ…å«æ›´å¤šç›¸å…³å…³é”®è¯",
                code_solution=f'<title>{title_text} - è¡¥å……å…³é”®è¯</title>',
                impact_score=5.0
            ))
        elif title_length > 60:
            self.issues.append(SEOIssue(
                category="Metaæ ‡ç­¾",
                severity="medium",
                title="Titleæ ‡ç­¾è¿‡é•¿",
                description=f"Titleæ ‡ç­¾é•¿åº¦ä¸º {title_length} å­—ç¬¦ï¼Œè¶…è¿‡æ¨èçš„60å­—ç¬¦",
                recommendation="ç¼©çŸ­Titleæ ‡ç­¾ï¼Œç¡®ä¿é‡è¦ä¿¡æ¯åœ¨æœç´¢ç»“æœä¸­å®Œæ•´æ˜¾ç¤º",
                code_solution=f'<title>{title_text[:57]}...</title>',
                impact_score=3.0
            ))

        self.metrics.append(SEOMetric(
            name="Titleé•¿åº¦",
            current_value=title_length,
            target_value=50,
            unit="å­—ç¬¦",
            status="good" if 30 <= title_length <= 60 else "warning"
        ))

    def check_meta_description(self, soup: BeautifulSoup, url: str) -> None:
        """æ£€æŸ¥Meta Description"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})

        if not meta_desc or not meta_desc.get('content'):
            self.issues.append(SEOIssue(
                category="Metaæ ‡ç­¾",
                severity="high",
                title="ç¼ºå°‘Meta Description",
                description=f"é¡µé¢ {url} æ²¡æœ‰Meta Description",
                recommendation="æ·»åŠ Meta Descriptionï¼Œé•¿åº¦æ§åˆ¶åœ¨150-160å­—ç¬¦ï¼ŒåŒ…å«å…³é”®è¯å’Œè¡ŒåŠ¨å¬å”¤",
                code_solution='<meta name="description" content="é¡µé¢æè¿°ï¼ŒåŒ…å«å…³é”®è¯å’Œè¡ŒåŠ¨å¬å”¤">',
                impact_score=8.0
            ))
            return

        desc_content = meta_desc.get('content', '').strip()
        desc_length = len(desc_content)

        if desc_length < 120:
            self.issues.append(SEOIssue(
                category="Metaæ ‡ç­¾",
                severity="medium",
                title="Meta Descriptionè¿‡çŸ­",
                description=f"Meta Descriptioné•¿åº¦ä¸º {desc_length} å­—ç¬¦ï¼Œå°‘äºæ¨èçš„120å­—ç¬¦",
                recommendation="å¢åŠ æè¿°é•¿åº¦ï¼Œæ›´å¥½åœ°æè¿°é¡µé¢å†…å®¹",
                impact_score=3.0
            ))
        elif desc_length > 160:
            self.issues.append(SEOIssue(
                category="Metaæ ‡ç­¾",
                severity="medium",
                title="Meta Descriptionè¿‡é•¿",
                description=f"Meta Descriptioné•¿åº¦ä¸º {desc_length} å­—ç¬¦ï¼Œè¶…è¿‡æ¨èçš„160å­—ç¬¦",
                recommendation="ç¼©çŸ­æè¿°ï¼Œç¡®ä¿åœ¨æœç´¢ç»“æœä¸­å®Œæ•´æ˜¾ç¤º",
                impact_score=2.0
            ))

        self.metrics.append(SEOMetric(
            name="Meta Descriptioné•¿åº¦",
            current_value=desc_length,
            target_value=150,
            unit="å­—ç¬¦",
            status="good" if 120 <= desc_length <= 160 else "warning"
        ))

    def check_heading_structure(self, soup: BeautifulSoup, url: str) -> None:
        """æ£€æŸ¥æ ‡é¢˜ç»“æ„"""
        h1_tags = soup.find_all('h1')

        if len(h1_tags) == 0:
            self.issues.append(SEOIssue(
                category="å†…å®¹ç»“æ„",
                severity="high",
                title="ç¼ºå°‘H1æ ‡ç­¾",
                description=f"é¡µé¢ {url} æ²¡æœ‰H1æ ‡ç­¾",
                recommendation="æ·»åŠ å”¯ä¸€çš„H1æ ‡ç­¾ï¼ŒåŒ…å«é¡µé¢ä¸»å…³é”®è¯",
                code_solution='<h1>é¡µé¢ä¸»æ ‡é¢˜</h1>',
                impact_score=9.0
            ))
        elif len(h1_tags) > 1:
            self.issues.append(SEOIssue(
                category="å†…å®¹ç»“æ„",
                severity="medium",
                title="å¤šä¸ªH1æ ‡ç­¾",
                description=f"é¡µé¢æœ‰ {len(h1_tags)} ä¸ªH1æ ‡ç­¾",
                recommendation="åªä¿ç•™ä¸€ä¸ªH1æ ‡ç­¾ï¼Œå…¶ä»–æ”¹ä¸ºH2æˆ–H3",
                impact_score=4.0
            ))

        # æ£€æŸ¥æ ‡é¢˜å±‚çº§ç»“æ„
        headings = []
        for i in range(1, 7):
            for heading in soup.find_all(f'h{i}'):
                headings.append((i, heading.get_text().strip()))

        # æ£€æŸ¥æ˜¯å¦æœ‰è·³çº§
        for i in range(1, len(headings)):
            current_level = headings[i][0]
            prev_level = headings[i-1][0]

            if current_level > prev_level + 1:
                self.issues.append(SEOIssue(
                    category="å†…å®¹ç»“æ„",
                    severity="low",
                    title="æ ‡é¢˜å±‚çº§è·³çº§",
                    description=f"H{prev_level} åç›´æ¥è·³åˆ° H{current_level}",
                    recommendation="ç¡®ä¿æ ‡é¢˜å±‚çº§è¿ç»­ï¼Œä¸è¦è·³çº§",
                    impact_score=1.0
                ))

        self.metrics.append(SEOMetric(
            name="H1æ ‡ç­¾æ•°é‡",
            current_value=len(h1_tags),
            target_value=1,
            unit="ä¸ª",
            status="good" if len(h1_tags) == 1 else "warning"
        ))

    def check_image_optimization(self, soup: BeautifulSoup, url: str) -> None:
        """æ£€æŸ¥å›¾ç‰‡ä¼˜åŒ–"""
        images = soup.find_all('img')
        images_without_alt = 0
        images_missing_lazy = 0

        for img in images:
            # æ£€æŸ¥altå±æ€§
            if not img.get('alt'):
                images_without_alt += 1

            # æ£€æŸ¥lazy loading
            if img.get('loading') != 'lazy':
                images_missing_lazy += 1

        if images_without_alt > 0:
            self.issues.append(SEOIssue(
                category="å›¾ç‰‡ä¼˜åŒ–",
                severity="high",
                title=f"ç¼ºå°‘Altå±æ€§çš„å›¾ç‰‡",
                description=f"æœ‰ {images_without_alt} ä¸ªå›¾ç‰‡ç¼ºå°‘altå±æ€§",
                recommendation="ä¸ºæ‰€æœ‰å›¾ç‰‡æ·»åŠ æè¿°æ€§çš„altå±æ€§ï¼ŒåŒ…å«ç›¸å…³å…³é”®è¯",
                code_solution='<img src="image.jpg" alt="å›¾ç‰‡æè¿°ï¼ŒåŒ…å«å…³é”®è¯" loading="lazy">',
                impact_score=6.0
            ))

        if images_missing_lazy > 0:
            self.issues.append(SEOIssue(
                category="å›¾ç‰‡ä¼˜åŒ–",
                severity="medium",
                title="æœªä½¿ç”¨æ‡’åŠ è½½",
                description=f"æœ‰ {images_missing_lazy} ä¸ªå›¾ç‰‡æœªä½¿ç”¨æ‡’åŠ è½½",
                recommendation="ä¸ºéé¦–å±å›¾ç‰‡æ·»åŠ loading="lazy"å±æ€§",
                code_solution='<img src="image.jpg" alt="æè¿°" loading="lazy">',
                impact_score=3.0
            ))

        alt_percentage = ((len(images) - images_without_alt) / len(images) * 100) if images else 0
        self.metrics.append(SEOMetric(
            name="å›¾ç‰‡Altå±æ€§è¦†ç›–ç‡",
            current_value=alt_percentage,
            target_value=100,
            unit="%",
            status="good" if alt_percentage >= 90 else "warning"
        ))

    def check_internal_links(self, soup: BeautifulSoup, url: str) -> None:
        """æ£€æŸ¥å†…éƒ¨é“¾æ¥"""
        links = soup.find_all('a', href=True)
        internal_links = 0
        external_links = 0
        links_without_text = 0

        for link in links:
            href = link.get('href', '')
            text = link.get_text().strip()

            if not text:
                links_without_text += 1
                continue

            # åˆ¤æ–­æ˜¯å¦ä¸ºå†…éƒ¨é“¾æ¥
            if href.startswith('/') or href.startswith(self.base_url):
                internal_links += 1
            elif href.startswith('http'):
                external_links += 1

        if links_without_text > 0:
            self.issues.append(SEOIssue(
                category="å†…éƒ¨é“¾æ¥",
                severity="medium",
                title="ç©ºé“¾æ¥æ–‡æœ¬",
                description=f"æœ‰ {links_without_text} ä¸ªé“¾æ¥æ²¡æœ‰æè¿°æ€§æ–‡æœ¬",
                recommendation="ä¸ºæ‰€æœ‰é“¾æ¥æ·»åŠ æè¿°æ€§æ–‡æœ¬",
                code_solution='<a href="page.html">æè¿°æ€§é“¾æ¥æ–‡æœ¬</a>',
                impact_score=3.0
            ))

        self.metrics.append(SEOMetric(
            name="å†…éƒ¨é“¾æ¥æ•°é‡",
            current_value=internal_links,
            target_value=10,
            unit="ä¸ª",
            status="good" if internal_links >= 5 else "warning"
        ))

    def check_meta_tags(self, soup: BeautifulSoup, url: str) -> None:
        """æ£€æŸ¥å…¶ä»–Metaæ ‡ç­¾"""
        essential_metas = ['viewport', 'robots', 'description']
        missing_metas = []

        for meta_name in essential_metas:
            if not soup.find('meta', attrs={'name': meta_name}):
                missing_metas.append(meta_name)

        if missing_metas:
            self.issues.append(SEOIssue(
                category="Metaæ ‡ç­¾",
                severity="high",
                title=f"ç¼ºå°‘é‡è¦çš„Metaæ ‡ç­¾",
                description=f"ç¼ºå°‘ä»¥ä¸‹Metaæ ‡ç­¾: {', '.join(missing_metas)}",
                recommendation="æ·»åŠ ç¼ºå°‘çš„Metaæ ‡ç­¾",
                code_solution='''
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="robots" content="index, follow">
<meta name="description" content="é¡µé¢æè¿°">
                ''',
                impact_score=7.0
            ))

        # æ£€æŸ¥Open Graphæ ‡ç­¾
        og_tags = ['og:title', 'og:description', 'og:image', 'og:url']
        missing_og = []

        for og_tag in og_tags:
            if not soup.find('meta', attrs={'property': og_tag}):
                missing_og.append(og_tag)

        if missing_og:
            self.issues.append(SEOIssue(
                category="ç¤¾äº¤åª’ä½“",
                severity="medium",
                title="ç¼ºå°‘Open Graphæ ‡ç­¾",
                description=f"ç¼ºå°‘ä»¥ä¸‹OGæ ‡ç­¾: {', '.join(missing_og)}",
                recommendation="æ·»åŠ Open Graphæ ‡ç­¾ä»¥ä¼˜åŒ–ç¤¾äº¤åª’ä½“åˆ†äº«æ•ˆæœ",
                code_solution='''
<meta property="og:title" content="é¡µé¢æ ‡é¢˜">
<meta property="og:description" content="é¡µé¢æè¿°">
<meta property="og:image" content="åˆ†äº«å›¾ç‰‡URL">
<meta property="og:url" content="é¡µé¢URL">
                ''',
                impact_score=4.0
            ))

    def check_performance_hints(self, soup: BeautifulSoup, url: str) -> None:
        """æ£€æŸ¥æ€§èƒ½ç›¸å…³ä¼˜åŒ–"""
        # æ£€æŸ¥æ˜¯å¦æœ‰CSSé˜»å¡æ¸²æŸ“
        css_links = soup.find_all('link', attrs={'rel': 'stylesheet'})
        blocking_css = sum(1 for link in css_links if not link.get('media') or 'print' not in link.get('media', ''))

        if blocking_css > 2:
            self.issues.append(SEOIssue(
                category="æ€§èƒ½ä¼˜åŒ–",
                severity="medium",
                title="CSSé˜»å¡æ¸²æŸ“",
                description=f"æœ‰ {blocking_css} ä¸ªCSSæ–‡ä»¶å¯èƒ½é˜»å¡é¡µé¢æ¸²æŸ“",
                recommendation="ä½¿ç”¨åª’ä½“æŸ¥è¯¢æˆ–å¼‚æ­¥åŠ è½½éå…³é”®CSS",
                impact_score=3.0
            ))

        # æ£€æŸ¥æ˜¯å¦æœ‰å†…è”CSS
        inline_styles = soup.find_all('style')
        if inline_styles:
            self.issues.append(SEOIssue(
                category="æ€§èƒ½ä¼˜åŒ–",
                severity="low",
                title="å†…è”CSSæ ·å¼",
                description=f"é¡µé¢æœ‰ {len(inline_styles)} ä¸ªå†…è”æ ·å¼",
                recommendation="å°†CSSç§»è‡³å¤–éƒ¨æ–‡ä»¶ï¼Œå‡å°‘HTMLä½“ç§¯",
                impact_score=1.0
            ))

    def analyze_page_seo(self, url: str) -> None:
        """åˆ†æå•ä¸ªé¡µé¢çš„SEO"""
        soup = self.analyze_page(url)
        if not soup:
            return

        # æ‰§è¡Œå„é¡¹æ£€æŸ¥
        self.check_title_tags(soup, url)
        self.check_meta_description(soup, url)
        self.check_heading_structure(soup, url)
        self.check_image_optimization(soup, url)
        self.check_internal_links(soup, url)
        self.check_meta_tags(soup, url)
        self.check_performance_hints(soup, url)

    def analyze_sitemap(self) -> None:
        """åˆ†æç½‘ç«™åœ°å›¾"""
        sitemap_urls = [
            f"{self.base_url}/sitemap.xml",
            f"{self.base_url}/sitemap_index.xml"
        ]

        sitemap_found = False
        for sitemap_url in sitemap_urls:
            try:
                response = self.session.get(sitemap_url, timeout=10)
                if response.status_code == 200:
                    sitemap_found = True
                    print(f"âœ… æ‰¾åˆ°Sitemap: {sitemap_url}")
                    break
            except:
                continue

        if not sitemap_found:
            self.issues.append(SEOIssue(
                category="æŠ€æœ¯SEO",
                severity="medium",
                title="ç¼ºå°‘Sitemap",
                description="æœªæ‰¾åˆ°sitemap.xmlæ–‡ä»¶",
                recommendation="åˆ›å»ºå¹¶æäº¤sitemap.xmlåˆ°æœç´¢å¼•æ“",
                impact_score=5.0
            ))

    def analyze_robots_txt(self) -> None:
        """åˆ†ærobots.txt"""
        robots_url = f"{self.base_url}/robots.txt"

        try:
            response = self.session.get(robots_url, timeout=10)
            if response.status_code == 200:
                print("âœ… æ‰¾åˆ°robots.txt")

                # æ£€æŸ¥æ˜¯å¦å…è®¸æœç´¢å¼•æ“è®¿é—®
                content = response.text.lower()
                if 'disallow: /' in content and 'allow:' not in content:
                    self.issues.append(SEOIssue(
                        category="æŠ€æœ¯SEO",
                        severity="critical",
                        title="Robots.txté˜»æ­¢æ‰€æœ‰è®¿é—®",
                        description="robots.txtå¯èƒ½é˜»æ­¢æœç´¢å¼•æ“è®¿é—®ç½‘ç«™",
                        recommendation="æ£€æŸ¥robots.txté…ç½®ï¼Œç¡®ä¿å…è®¸æœç´¢å¼•æ“è®¿é—®é‡è¦é¡µé¢",
                        impact_score=10.0
                    ))
            else:
                self.issues.append(SEOIssue(
                    category="æŠ€æœ¯SEO",
                    severity="medium",
                    title="ç¼ºå°‘robots.txt",
                    description="æœªæ‰¾åˆ°robots.txtæ–‡ä»¶",
                    recommendation="åˆ›å»ºrobots.txtæ–‡ä»¶ï¼ŒæŒ‡å¯¼æœç´¢å¼•æ“çˆ¬å–",
                    code_solution='''User-agent: *
Allow: /
Sitemap: https://yoursite.com/sitemap.xml''',
                    impact_score=3.0
                ))
        except:
            self.issues.append(SEOIssue(
                category="æŠ€æœ¯SEO",
                severity="medium",
                title="æ— æ³•è®¿é—®robots.txt",
                description="æ— æ³•è®¿é—®robots.txtæ–‡ä»¶",
                recommendation="ç¡®ä¿robots.txtæ–‡ä»¶å¯ä»¥æ­£å¸¸è®¿é—®",
                impact_score=2.0
            ))

    def generate_optimization_plan(self) -> None:
        """ç”Ÿæˆä¼˜åŒ–è®¡åˆ’"""
        print("\n" + "="*80)
        print("ğŸ“‹ SEOä¼˜åŒ–åˆ†ææŠ¥å‘Š")
        print("="*80)

        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºé—®é¢˜
        critical_issues = [i for i in self.issues if i.severity == 'critical']
        high_issues = [i for i in self.issues if i.severity == 'high']
        medium_issues = [i for i in self.issues if i.severity == 'medium']
        low_issues = [i for i in self.issues if i.severity == 'low']

        total_impact = sum(i.impact_score for i in self.issues)

        print(f"ğŸ“Š å‘ç°é—®é¢˜æ€»æ•°: {len(self.issues)} ä¸ª")
        print(f"ğŸš¨ ä¸¥é‡é—®é¢˜: {len(critical_issues)} ä¸ª")
        print(f"âš ï¸  é«˜ä¼˜å…ˆçº§: {len(high_issues)} ä¸ª")
        print(f"ğŸ“ ä¸­ç­‰ä¼˜å…ˆçº§: {len(medium_issues)} ä¸ª")
        print(f"ğŸ’¡ ä½ä¼˜å…ˆçº§: {len(low_issues)} ä¸ª")
        print(f"ğŸ¯ é¢„ä¼°SEOè¯„åˆ†æå‡: +{total_impact:.1f} åˆ†")

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        print("\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
        for metric in self.metrics:
            status_icon = "âœ…" if metric.status == "good" else "âš ï¸"
            print(f"   {status_icon} {metric.name}: {metric.current_value:.1f}{metric.unit} (ç›®æ ‡: {metric.target_value}{metric.unit})")

        # æ˜¾ç¤ºé—®é¢˜è¯¦æƒ…
        if critical_issues or high_issues:
            print(f"\nğŸš¨ éœ€è¦ä¼˜å…ˆå¤„ç†çš„é—®é¢˜:")
            for issue in critical_issues + high_issues:
                print(f"\n   [{issue.severity.upper()}] {issue.title}")
                print(f"   ğŸ“ {issue.description}")
                print(f"   ğŸ’¡ å»ºè®®: {issue.recommendation}")

        # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
        if self.issues:
            print(f"\nğŸ¯ æ¨èçš„ä¼˜åŒ–ç­–ç•¥:")

            # æ ¹æ®é—®é¢˜ç±»å‹æ¨èä¼˜åŒ–ç­–ç•¥
            categories = list(set(issue.category for issue in self.issues))
            for category in categories:
                category_issues = [i for i in self.issues if i.category == category]
                print(f"\n   ğŸ“‚ {category} ({len(category_issues)} ä¸ªé—®é¢˜):")

                for issue in category_issues[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"      â€¢ {issue.title}")

    def generate_code_solutions(self) -> str:
        """ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆ"""
        if not self.issues:
            return "æ­å–œï¼æœªå‘ç°SEOé—®é¢˜ã€‚"

        solutions = []
        solutions.append("# SEOä¼˜åŒ–ä»£ç è§£å†³æ–¹æ¡ˆ\n")
        solutions.append("```html\n")

        # æŒ‰ç±»åˆ«åˆ†ç»„ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
        categories = {}
        for issue in self.issues:
            if issue.category not in categories:
                categories[issue.category] = []
            categories[issue.category].append(issue)

        for category, issues in categories.items():
            solutions.append(f"\n<!-- {category} ä¼˜åŒ– -->")

            # å»é‡ä»£ç è§£å†³æ–¹æ¡ˆ
            unique_solutions = set()
            for issue in issues:
                if issue.code_solution and issue.code_solution not in unique_solutions:
                    unique_solutions.add(issue.code_solution)
                    solutions.append(f"<!-- {issue.title} -->")
                    solutions.append(issue.code_solution)

        solutions.append("\n```")
        return "\n".join(solutions)

    def save_report(self, filename: str = None) -> None:
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        if not filename:
            timestamp = self.analysis_timestamp.strftime("%Y%m%d_%H%M%S")
            filename = f"seo_report_{timestamp}.md"

        report_content = []
        report_content.append("# SEOåˆ†ææŠ¥å‘Š\n")
        report_content.append(f"**ç½‘ç«™**: {self.base_url}\n")
        report_content.append(f"**åˆ†ææ—¶é—´**: {self.analysis_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n")

        # é—®é¢˜æ±‡æ€»
        critical_issues = [i for i in self.issues if i.severity == 'critical']
        high_issues = [i for i in self.issues if i.severity == 'high']

        report_content.append("## é—®é¢˜æ±‡æ€»\n")
        report_content.append(f"- æ€»é—®é¢˜æ•°: {len(self.issues)}\n")
        report_content.append(f"- ä¸¥é‡é—®é¢˜: {len(critical_issues)}\n")
        report_content.append(f"- é«˜ä¼˜å…ˆçº§: {len(high_issues)}\n")

        # è¯¦ç»†é—®é¢˜åˆ—è¡¨
        report_content.append("## è¯¦ç»†é—®é¢˜\n")
        for i, issue in enumerate(self.issues, 1):
            report_content.append(f"### {i}. {issue.title}\n")
            report_content.append(f"**ä¸¥é‡ç¨‹åº¦**: {issue.severity}\n")
            report_content.append(f"**æè¿°**: {issue.description}\n")
            report_content.append(f"**å»ºè®®**: {issue.recommendation}\n")
            if issue.code_solution:
                report_content.append(f"**ä»£ç è§£å†³æ–¹æ¡ˆ**:\n```html\n{issue.code_solution}\n```\n")

        # æ·»åŠ ä»£ç è§£å†³æ–¹æ¡ˆ
        report_content.append("## ä»£ç è§£å†³æ–¹æ¡ˆ\n")
        report_content.append(self.generate_code_solutions())

        # ä¿å­˜æ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))

        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

    def interactive_optimization(self) -> None:
        """äº¤äº’å¼ä¼˜åŒ–æµç¨‹"""
        print("\n" + "="*80)
        print("ğŸ¤– å¼€å§‹äº¤äº’å¼SEOä¼˜åŒ–æµç¨‹")
        print("="*80)

        if not self.issues:
            print("ğŸ‰ æ­å–œï¼æœªå‘ç°SEOé—®é¢˜ã€‚")
            return

        # è¯¢é—®ç”¨æˆ·è¦ä¼˜åŒ–çš„é—®é¢˜ç±»å‹
        categories = list(set(issue.category for issue in self.issues))
        category_options = ["å…¨éƒ¨é—®é¢˜"] + categories + ["ç”Ÿæˆå®Œæ•´æŠ¥å‘Š", "é€€å‡º"]

        choice_idx = self.get_user_choice("è¯·é€‰æ‹©è¦ä¼˜åŒ–çš„é—®é¢˜ç±»å‹:", category_options)

        if choice_idx == len(category_options) - 1:  # é€€å‡º
            return
        elif choice_idx == len(category_options) - 2:  # ç”ŸæˆæŠ¥å‘Š
            self.save_report()
            return
        elif choice_idx == 0:  # å…¨éƒ¨é—®é¢˜
            selected_issues = self.issues
        else:  # ç‰¹å®šç±»åˆ«
            selected_category = categories[choice_idx - 1]
            selected_issues = [i for i in self.issues if i.category == selected_category]

        # æ˜¾ç¤ºé€‰ä¸­çš„é—®é¢˜
        print(f"\nğŸ“‹ é€‰ä¸­äº† {len(selected_issues)} ä¸ªé—®é¢˜:")
        for i, issue in enumerate(selected_issues, 1):
            print(f"   {i}. {issue.title} ({issue.severity})")

        # ç¡®è®¤æ˜¯å¦ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆ
        if self.get_user_confirmation(f"\nä¸ºè¿™ {len(selected_issues)} ä¸ªé—®é¢˜ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆ?"):
            print("\nğŸ’» æ­£åœ¨ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆ...")

            solutions = []
            for issue in selected_issues:
                if issue.code_solution:
                    solutions.append(f"<!-- {issue.title} -->")
                    solutions.append(issue.code_solution)
                    solutions.append("")

            if solutions:
                print("\n" + "="*60)
                print("ğŸ”§ ä»£ç è§£å†³æ–¹æ¡ˆ")
                print("="*60)
                print("\n".join(solutions))

                # è¯¢é—®æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶
                if self.get_user_confirmation("å°†ä»£ç è§£å†³æ–¹æ¡ˆä¿å­˜åˆ°æ–‡ä»¶?"):
                    with open("seo_solutions.html", 'w', encoding='utf-8') as f:
                        f.write("<!-- SEOä¼˜åŒ–ä»£ç è§£å†³æ–¹æ¡ˆ -->\n")
                        f.write("<!-- ç”±Claude SEO Agentç”Ÿæˆ -->\n")
                        f.write("\n".join(solutions))
                    print("âœ… ä»£ç è§£å†³æ–¹æ¡ˆå·²ä¿å­˜åˆ°: seo_solutions.html")
            else:
                print("âš ï¸  é€‰ä¸­çš„é—®é¢˜æ²¡æœ‰ä»£ç è§£å†³æ–¹æ¡ˆ")

    def run_analysis(self, pages: List[str] = None) -> None:
        """è¿è¡Œå®Œæ•´çš„SEOåˆ†æ"""
        self.print_header()

        if not pages:
            pages = [self.base_url]

        # è¯¢é—®æ˜¯å¦åˆ†æå¤šä¸ªé¡µé¢
        if self.get_user_confirmation("æ˜¯å¦è¦åˆ†æå¤šä¸ªé¡µé¢? (å°†åˆ†æé¦–é¡µå’Œå¸¸è§é¡µé¢)"):
            # å¸¸è§é¡µé¢æ¨¡å¼
            common_patterns = [
                "/about", "/contact", "/products", "/services",
                "/blog", "/news", "/help", "/faq"
            ]

            for pattern in common_patterns:
                test_url = self.base_url + pattern
                try:
                    response = self.session.head(test_url, timeout=5)
                    if response.status_code == 200:
                        pages.append(test_url)
                except:
                    continue

        print(f"\nğŸ” å¼€å§‹åˆ†æ {len(pages)} ä¸ªé¡µé¢...")

        # åˆ†ææ¯ä¸ªé¡µé¢
        for page in pages:
            self.analyze_page_seo(page)
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

        # æŠ€æœ¯SEOæ£€æŸ¥
        print("\nğŸ”§ æ£€æŸ¥æŠ€æœ¯SEO...")
        self.analyze_robots_txt()
        self.analyze_sitemap()

        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.generate_optimization_plan()

        # äº¤äº’å¼ä¼˜åŒ–
        while True:
            print("\n" + "="*80)
            print("ğŸ¤– Claude SEO Agent - å¯ç”¨æ“ä½œ")
            print("="*80)
            options = [
                "æŸ¥çœ‹è¯¦ç»†é—®é¢˜",
                "ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆ",
                "ä¿å­˜å®Œæ•´æŠ¥å‘Š",
                "é‡æ–°åˆ†æ",
                "é€€å‡º"
            ]

            choice_idx = self.get_user_choice("è¯·é€‰æ‹©æ“ä½œ:", options)

            if choice_idx == 0:  # æŸ¥çœ‹è¯¦ç»†é—®é¢˜
                print("\nğŸ“‹ è¯¦ç»†é—®é¢˜åˆ—è¡¨:")
                for i, issue in enumerate(self.issues, 1):
                    print(f"\n{i}. [{issue.severity.upper()}] {issue.title}")
                    print(f"   ğŸ“ {issue.description}")
                    print(f"   ğŸ’¡ {issue.recommendation}")
                    if issue.code_solution:
                        print(f"   ğŸ”§ {issue.code_solution}")

            elif choice_idx == 1:  # ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆ
                print(self.generate_code_solutions())

            elif choice_idx == 2:  # ä¿å­˜æŠ¥å‘Š
                self.save_report()

            elif choice_idx == 3:  # é‡æ–°åˆ†æ
                print("\nğŸ”„ é‡æ–°å¼€å§‹åˆ†æ...")
                self.issues.clear()
                self.metrics.clear()
                self.run_analysis(pages)
                return

            elif choice_idx == 4:  # é€€å‡º
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨Claude SEO Agent!")
                break

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– Claude SEO Agent - AIé©±åŠ¨çš„SEOåˆ†æå’Œä¼˜åŒ–åŠ©æ‰‹")
    print("="*80)

    # è·å–ç”¨æˆ·è¾“å…¥çš„ç½‘ç«™URL
    while True:
        url = input("\nè¯·è¾“å…¥è¦åˆ†æçš„ç½‘ç«™URL (ä¾‹å¦‚: https://example.com): ").strip()

        if not url:
            print("âš ï¸  è¯·è¾“å…¥æœ‰æ•ˆçš„URL")
            continue

        # æ·»åŠ åè®®å‰ç¼€
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        # éªŒè¯URLæ ¼å¼
        try:
            parsed = urlparse(url)
            if not parsed.netloc:
                print("âš ï¸  è¯·è¾“å…¥æœ‰æ•ˆçš„URLæ ¼å¼")
                continue

            # æµ‹è¯•è¿æ¥
            test_response = requests.head(url, timeout=10)
            if test_response.status_code not in [200, 403, 404]:
                print(f"âš ï¸  æ— æ³•è®¿é—®ç½‘ç«™ (HTTP {test_response.status_code})")
                continue

            break

        except Exception as e:
            print(f"âš ï¸  è¿æ¥å¤±è´¥: {str(e)}")
            continue

    # åˆ›å»ºå¹¶è¿è¡ŒSEO Agent
    agent = SEOOptimizerAgent(url)

    try:
        agent.run_analysis()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ åˆ†æå·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()